# Deep Learning with Transformers

<figure><img src="https://lh3.googleusercontent.com/y1mJapJxVF7MbozonPMzKR_hF3m3sTc0-pKWt5Qb_4oFLoepReHFv6tSdrzaVgNNm8ktE47J9HCvcFCWUXP4BHN4jVcjEBPpKLRusPZKaJ2QchM1QXUKj3ixvfaXH437co-aEHudYjPE3kjtZlUQMUCSc4v4Cubn2KqsLORKdjDqol5pLttZLjLPDfRluA" alt=""><figcaption></figcaption></figure>

{% content-ref url="paradigms-of-deep-learning-research.md" %}
[paradigms-of-deep-learning-research.md](paradigms-of-deep-learning-research.md)
{% endcontent-ref %}

{% content-ref url="sequence-modelling-with-transformer-encoder.md" %}
[sequence-modelling-with-transformer-encoder.md](sequence-modelling-with-transformer-encoder.md)
{% endcontent-ref %}

{% content-ref url="sequence-generation-with-transformer-decoder.md" %}
[sequence-generation-with-transformer-decoder.md](sequence-generation-with-transformer-decoder.md)
{% endcontent-ref %}

{% content-ref url="sequence-to-sequence-generation-with-transformer-encoder-decoder.md" %}
[sequence-to-sequence-generation-with-transformer-encoder-decoder.md](sequence-to-sequence-generation-with-transformer-encoder-decoder.md)
{% endcontent-ref %}

{% content-ref url="self-supervised-pretraining-of-transformers.md" %}
[self-supervised-pretraining-of-transformers.md](self-supervised-pretraining-of-transformers.md)
{% endcontent-ref %}

{% content-ref url="speeding-up-transformers.md" %}
[speeding-up-transformers.md](speeding-up-transformers.md)
{% endcontent-ref %}

{% content-ref url="case-studies-of-transformer-models/" %}
[case-studies-of-transformer-models](case-studies-of-transformer-models/)
{% endcontent-ref %}

{% content-ref url="finetuning-large-language-models/" %}
[finetuning-large-language-models](finetuning-large-language-models/)
{% endcontent-ref %}

{% content-ref url="vision-transformers.md" %}
[vision-transformers.md](vision-transformers.md)
{% endcontent-ref %}

{% content-ref url="speech-transformers.md" %}
[speech-transformers.md](speech-transformers.md)
{% endcontent-ref %}

