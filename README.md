# Deep Learning with Transformers

<figure><img src=".gitbook/assets/transformers_book.jpg" alt=""><figcaption></figcaption></figure>

## PREFACE

The turing test proposed by Alan turing in his seminal paper, "Computing Machinery and Intelligence." published in 1950, states that a true test of intelligence is if a machine can give a response which we cannot tell differently from responses given by humans. Researchers spent decades trying to build such systems, that can speak and write like humans. Today, we are faced with a different problem. Thanks to advances in deep learning, Large language models such as GPT 4 are able to generate texts, answer essay questions and even take exams with results that are often indistinguishable from humans. Teachers in schools today are having to deal with students letting AI models do their essays and programmers are regularly using AI services as pair programmers. Similar phenomenon is being observed in the computer vision space with generative models like Midjourney, Dalle and stable diffusion creating artworks and digital images that look like 4K photography.&#x20;

<figure><img src=".gitbook/assets/astronaut_feeding_chickens.png" alt=""><figcaption><p>Astronaut Feeding chickens (Generated by StableDiffusion)</p></figcaption></figure>

The image above shows the power of text to image generative models. In this instance, given the text description "Astronaut Feeding Chickens", the stable diffusion model generated a high quality image that feels so real.

While it has taken many decades to go from Turing's paper to the current state of AI, the pace of AI development has accelerated massively since the year 2012.&#x20;



For decades, computer scientists attempted to build AI powered computer vision, speech processing and text processing systems. While some of these attempts were successfully applied in a number of applications and industries, the field of artificial intelligence remained a niche space that was more fiction than reality, as the methods developed then were very limited in their capabilities, brittle and barely worked for most practical applications.

This all changed in 2012 when a group of computer scientists led by Alex Krizhevsky, a PHD student at the University of Toronto, developed a image recognition model called "AlexNet."

The remarkable accuracy archieved by AlexNet triggered a massive revolution in the field of artificial intelligence. Key to the success of AlexNet were the availability of high performance GPU hardware from Nvidia and the large scale data on the internet, from which the Imagenet dataset was curated. The actual algorithm itself was a convolutional neural network based on earlier work by Yan Lecun in 1989.

A lot of companies and academic institutions quickly realized the huge potential of this breakthrough, therefore the next couple of years saw the rise of more powerful models, better GPUs from Nvidia and bigger datasets curated from the world wide web. These three developments worked in tandem to unleash rapid advances in object detection and recognition, language processing and speech processing.&#x20;

Earlier attempts to build AI systems for vision, speech and text achieved little success, however, in 2012, the transition to deep learning models began after the massive success achieved in the image recognition space by  Although, deep learning had existed long before then, in 2012, the availability of big data from the internet and high performance GPU hardware and the CUDA HPC framework from Nvidia made enabled the true potentials of deep learning to be realized. From there on, AI research exploded.&#x20;

From then till 2017, almost all of the research was focussed on finding clever ways to structure convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to solve new problems, achieve better accuracies and run faster. CNNs and RNNs are two foundational architectures that had been invented prior to the big data, big compute era, however, in 2017, Vaswani et al proposed a whole new type of architecture called "Transformers" in the paper "Attention is All You Need". This new foundational architecture completely revolutionized the way we build deep learning models and enabled new levels of accuracy and capabilities that represented  giant leap in the step to building highly intelligent systems.

Fast forward to 2023, computer vision, language processing and speech processing have become completely dominated by transformer models.

So here we are, Alan Turing's dreams finally getting realized using large language models built with transformers.&#x20;

However, despite their powerful capabilities and popularity, knowledge about how transformers work is still limited. The way they work is simpler than prior models, however, they are still new.

In this book, we shall explain in great detail every aspect of the transformer architecture with simple to understand illustrations, code samples, diagrams and equations. As much as possible, we shall keep the maths to the bearest minimum. A basic knowledge of matrix multiplications should be sufficient to get through this book. This book is aimed at both researchers and developers.

While we do not attempt to make this an encyclopaedia of all there is in the rather large literature of transformer models, we have included chapters on optimization of transformers, finetuning and specialization and applications of transformers to domains such as vision and speech in addition to natural language processing which we extensively cover.

We hope this book helps you become a better researcher and improves your understanding of the amazing models developed in this space over the past few  years.

## Table of Contents

{% content-ref url="paradigms-of-deep-learning-research.md" %}
[paradigms-of-deep-learning-research.md](paradigms-of-deep-learning-research.md)
{% endcontent-ref %}

{% content-ref url="sequence-modelling-with-transformer-encoder.md" %}
[sequence-modelling-with-transformer-encoder.md](sequence-modelling-with-transformer-encoder.md)
{% endcontent-ref %}

{% content-ref url="sequence-generation-with-transformer-decoder.md" %}
[sequence-generation-with-transformer-decoder.md](sequence-generation-with-transformer-decoder.md)
{% endcontent-ref %}

{% content-ref url="sequence-to-sequence-generation-with-transformer-encoder-decoder.md" %}
[sequence-to-sequence-generation-with-transformer-encoder-decoder.md](sequence-to-sequence-generation-with-transformer-encoder-decoder.md)
{% endcontent-ref %}

{% content-ref url="self-supervised-pretraining-of-transformers.md" %}
[self-supervised-pretraining-of-transformers.md](self-supervised-pretraining-of-transformers.md)
{% endcontent-ref %}

{% content-ref url="speeding-up-transformers.md" %}
[speeding-up-transformers.md](speeding-up-transformers.md)
{% endcontent-ref %}

{% content-ref url="case-studies-of-transformer-models/" %}
[case-studies-of-transformer-models](case-studies-of-transformer-models/)
{% endcontent-ref %}

{% content-ref url="finetuning-large-language-models/" %}
[finetuning-large-language-models](finetuning-large-language-models/)
{% endcontent-ref %}

{% content-ref url="vision-transformers.md" %}
[vision-transformers.md](vision-transformers.md)
{% endcontent-ref %}

{% content-ref url="speech-transformers.md" %}
[speech-transformers.md](speech-transformers.md)
{% endcontent-ref %}

